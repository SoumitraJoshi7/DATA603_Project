{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f453ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, col, udf, row_number,avg, desc\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql import Window\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH, Normalizer\n",
    "import plotly.express as px\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6e9c9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/09 18:23:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data Preprocessing\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cc6a15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets from hadoop\n",
    "file_path1 = \"hdfs://localhost:9000/datafolder/datazip/Books_rating.csv\"\n",
    "file_path2 = \"hdfs://localhost:9000/datafolder/datazip/books_data.csv\"\n",
    "\n",
    "books_rating = spark.read.csv(file_path1, header=True, inferSchema=True)\n",
    "books_data = spark.read.csv(file_path2, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e60367c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/09 18:23:57 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-------------------+------------------+--------------------+\n",
      "|summary|                  Id|               Title|            User_id|      review/score|               Price|\n",
      "+-------+--------------------+--------------------+-------------------+------------------+--------------------+\n",
      "|  count|             3000000|             2999792|            2437750|           2999870|              482421|\n",
      "|   mean|1.0568515696607149E9|   2012.796651763537|  18.29299003322259| 1656.860421970827|  21.767951161877054|\n",
      "| stddev| 1.284488524833734E9|  1536.7533549608797|  21.99284402625621|1427549.9863179324|   26.21155241772817|\n",
      "|    min|          0001047604|  \"\"\" Film technique| \"\" Film acting \"\"\"|   & Algorithms\"\"\"|              \"\" and|\n",
      "|    max|          B0064P287I|you can do anythi...|      AZZZZW74AAX75|         thersites|: A guide to loca...|\n",
      "+-------+--------------------+--------------------+-------------------+------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|summary|               Title|             authors|           publisher|          categories|       publishedDate|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  count|              212403|              181153|              139274|              171880|              186560|\n",
      "|   mean|   3823.672941176471|              1578.4|             3734.75|  1983.7334777898159|   1982.702933143332|\n",
      "| stddev|  10717.999589636447|  1278.7901502106834|  10193.316327911616|  142.43423125699238|   37.65620052385513|\n",
      "|    min|  \"\"\" Film technique| \"\" \"\"I'm a Littl...| \"\" \"\"Skipper Ire...| \"\" Knox's quirky...| \"\" \"\"Cruising fo...|\n",
      "|    max|you can do anythi...|” “Jeanie with th...|                펜립|�� folk art is a ...|” which is anthol...|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:=======>                                                   (1 + 7) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Considering only the required columns\n",
    "books_rating = books_rating[['Id','Title','User_id','review/score','Price']] \n",
    "books_data = books_data[['Title','authors','publisher','categories','publishedDate']]\n",
    "\n",
    "books_rating.describe().show()\n",
    "books_data.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6434bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the year from the publishedDate column\n",
    "from pyspark.sql.functions import year\n",
    "books_data = books_data.withColumn(\"publishedYear\", year(\"publishedDate\")).drop(\"publishedDate\")\n",
    "books_data = books_data.filter(col(\"publishedYear\").rlike(\"^\\d+$\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8ccc76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of duplicates in books_rating: 0\n",
      "number of duplicates in books_data: 0\n"
     ]
    }
   ],
   "source": [
    "# dropping duplicate rows in both the tables\n",
    "\n",
    "count_original = books_rating.count() \n",
    "books_rating.dropDuplicates()\n",
    "count_after = books_rating.count() \n",
    "d1 = count_original - count_after\n",
    "print(\"number of duplicates in books_rating:\", d1)\n",
    "count_original2 = books_data.count() \n",
    "books_data.dropDuplicates()\n",
    "count_after2 = books_data.count() \n",
    "d2 = count_original2 - count_after2\n",
    "print(\"number of duplicates in books_data:\", d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7583d7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=38790Kb max_used=38899Kb free=92281Kb\n",
      " bounds [0x00000001020b0000, 0x0000000104700000, 0x000000010a0b0000]\n",
      " total_blobs=14222 nmethods=13280 adapters=854\n",
      " compilation: disabled (not enough contiguous free space left)\n",
      "Null values in books_rating dataset: {'Id': 0, 'Title': 208, 'User_id': 562250, 'review/score': 130, 'Price': 2517579}\n",
      "Null values in books_data dataset: {'Title': 1, 'authors': 6839, 'publisher': 48179, 'categories': 15259, 'publishedYear': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to display the count of null values for each column\n",
    "null_counts_rating = {col_name: books_rating.filter(col(col_name).isNull()).count() for col_name in books_rating.columns}\n",
    "null_counts_data = {col_name: books_data.filter(col(col_name).isNull()).count() for col_name in books_data.columns}\n",
    "print(\"Null values in books_rating dataset:\", null_counts_rating)\n",
    "print(\"Null values in books_data dataset:\", null_counts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb92c748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+------------+-----+\n",
      "|        Id|               Title|       User_id|review/score|Price|\n",
      "+----------+--------------------+--------------+------------+-----+\n",
      "|1882931173|Its Only Art If I...| AVCGYZL8FQQTD|         4.0| NULL|\n",
      "|0826414346|Dr. Seuss: Americ...|A30TK6U7DNS82R|         5.0| NULL|\n",
      "|0826414346|Dr. Seuss: Americ...|A3UH4UZ4RSVO82|         5.0| NULL|\n",
      "|0826414346|Dr. Seuss: Americ...|A2MVUWT453QH61|         4.0| NULL|\n",
      "|0826414346|Dr. Seuss: Americ...|A22X4XUPKF66MR|         4.0| NULL|\n",
      "|0826414346|Dr. Seuss: Americ...|A2F6NONFUDB6UK|         4.0| NULL|\n",
      "|0826414346|Dr. Seuss: Americ...|A14OJS0VWMOSWO|         5.0| NULL|\n",
      "|0826414346|Dr. Seuss: Americ...|A2RSSXTDZDUSH4|         5.0| NULL|\n",
      "|0826414346|Dr. Seuss: Americ...|A25MD5I2GUIW6W|         5.0| NULL|\n",
      "|0826414346|Dr. Seuss: Americ...|A3VA4XFS5WNJO3|         4.0| NULL|\n",
      "|0829814000|Wonderful Worship...| AZ0IOBU20TBOP|         5.0|19.40|\n",
      "|0829814000|Wonderful Worship...|A373VVEU6Z9M0N|         5.0|19.40|\n",
      "|0829814000|Wonderful Worship...| AGKGOH65VTRR4|         5.0|19.40|\n",
      "|0829814000|Wonderful Worship...| A3OQWLU31BU1Y|         5.0|19.40|\n",
      "|0595344550|Whispers of the W...|A3Q12RK71N74LB|         1.0|10.95|\n",
      "|0595344550|Whispers of the W...|A1E9M6APK30ZAU|         4.0|10.95|\n",
      "|0595344550|Whispers of the W...| AUR0VA5H0C66C|         1.0|10.95|\n",
      "|0595344550|Whispers of the W...|A1YLDZ3VHR6QPZ|         5.0|10.95|\n",
      "|0595344550|Whispers of the W...| ACO23CG8K8T77|         5.0|10.95|\n",
      "|0595344550|Whispers of the W...|A1VK81CRRC7MLM|         5.0|10.95|\n",
      "+----------+--------------------+--------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col, first\n",
    "from pyspark.sql.window import Window\n",
    "books_rating = books_rating.withColumn(\"Price\", when(col(\"Price\").cast(\"double\").isNotNull(), col(\"Price\")).otherwise(None))\n",
    "books_rating.show()\n",
    "window_spec = Window.partitionBy(\"Title\").orderBy(\"Price\")\n",
    "books_rating = books_rating.withColumn(\"Price\", first(\"Price\", True).over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e64b52c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in books_rating dataset: {'Id': 0, 'Title': 0, 'User_id': 0, 'review/score': 0, 'Price': 0}\n",
      "Null values in books_data dataset: {'Title': 0, 'authors': 0, 'publisher': 0, 'categories': 0, 'publishedYear': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "|summary|                  Id|               Title|             User_id|        review/score|            Price|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "|  count|              414180|              414180|              414180|              414180|           414180|\n",
      "|   mean|1.0516446472336355E9|   1889.688679245283|                NULL|   4.239725294240715|21.62184188033609|\n",
      "| stddev|1.0441895933760347E9|   124.2255524061775|                NULL|  1.2911124349740133|26.24759579737497|\n",
      "|    min|          0002554232|\"\"\"Beauty Shop-Ph...|A00117421L76WVWG4...|     & Algorithms\"\"\"|             1.00|\n",
      "|    max|          B000TGB9VE|www.whitbread.org...|       AZZZZW74AAX75|teach to understa...|           995.00|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "\n",
      "+-------+--------------------+------------------------+--------------------+--------------------+-----------------+\n",
      "|summary|               Title|                 authors|           publisher|          categories|    publishedYear|\n",
      "+-------+--------------------+------------------------+--------------------+--------------------+-----------------+\n",
      "|  count|              112450|                  112450|              112450|              112450|           112450|\n",
      "|   mean|             1320.25|                    NULL|             51495.0|                NULL|2002.091943085816|\n",
      "| stddev|    841.971623206931|                    NULL|                NULL|                NULL|14.00102150533584|\n",
      "|    min|\"\"\" We'll Always ...|     The New York Tim...| as shared with D...| Luther Vandross....|             1016|\n",
      "|    max|you can do anythi...|['黒田領治', 'Ryōji K...|                펜립|          ['Zoning']|             2023|\n",
      "+-------+--------------------+------------------------+--------------------+--------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 104:===================================>                     (5 + 3) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Dropping null values\n",
    "\n",
    "books_rating = books_rating.na.drop()\n",
    "books_data = books_data.na.drop()\n",
    "null_counts_rating = {col_name: books_rating.filter(col(col_name).isNull()).count() for col_name in books_rating.columns}\n",
    "null_counts_data = {col_name: books_data.filter(col(col_name).isNull()).count() for col_name in books_data.columns}\n",
    "print(\"Null values in books_rating dataset:\", null_counts_rating)\n",
    "print(\"Null values in books_data dataset:\", null_counts_data)\n",
    "\n",
    "books_rating.describe().show()\n",
    "books_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06b29b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, split, size, regexp_replace, initcap\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "from pyspark.sql.functions import year\n",
    "def remove_special_characters(text):\n",
    "    if text is not None:\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "    else:\n",
    "        return None\n",
    "remove_special_characters_udf = udf(remove_special_characters, StringType())\n",
    "books_data = books_data.withColumn(\"categories\", remove_special_characters_udf(\"categories\"))\n",
    "books_data = books_data.filter(size(split(col(\"categories\"), \" \")) == 1)\n",
    "books_data = books_data.withColumn(\"Title\", regexp_replace(col(\"Title\"), \"\\\\b\\\\s+\", \"\"))\n",
    "books_data = books_data.withColumn(\"Title\", initcap(col(\"Title\")))\n",
    "books_rating = books_rating.withColumn(\"Title\", initcap(col(\"Title\")))\n",
    "books_data = books_data.withColumn(\"categories\", initcap(col(\"categories\")))\n",
    "def remove_integers_and_special_characters(text):\n",
    "    if text is not None:\n",
    "        return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    else:\n",
    "        return None\n",
    "remove_integers_and_special_characters_udf = udf(remove_integers_and_special_characters, StringType())\n",
    "books_data = books_data.withColumn(\"categories\", remove_integers_and_special_characters_udf(\"categories\"))\n",
    "books_data = books_data.filter(books_data[\"categories\"].rlike(\"^[a-zA-Z\\s]+$\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28708f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 109:======>                                                  (1 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "|summary|                  Id|               Title|             User_id|              rating|            Price|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "|  count|              414180|              414180|              414180|              414180|           414180|\n",
      "|   mean|1.0516446472336355E9|   1889.688679245283|                NULL|   4.239725294240715|21.62184188033609|\n",
      "| stddev|1.0441895933760347E9|   124.2255524061775|                NULL|  1.2911124349740133|26.24759579737497|\n",
      "|    min|          0002554232|\"\"\"beauty Shop-ph...|A00117421L76WVWG4...|     & Algorithms\"\"\"|             1.00|\n",
      "|    max|          B000TGB9VE|Zulu Shaman: Drea...|       AZZZZW74AAX75|teach to understa...|           995.00|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 109:==================================================>      (8 + 1) / 9]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Renaming columns for convenience\n",
    "books_rating = books_rating.withColumnRenamed(\"review/score\", \"rating\")\n",
    "books_rating.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82237e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|       User_id|count|\n",
      "+--------------+-----+\n",
      "|A14OJS0VWMOSWO| 2105|\n",
      "|   AFVQZQ8PW0L|  606|\n",
      "| AG35NEEFCMQVR|  307|\n",
      "|A1M8PP7MLHNBQB|  278|\n",
      "|A1D2C0WDCSHUWZ|  271|\n",
      "| AHD101501WCN1|  242|\n",
      "|A2VKWLCNZF4ZVB|  205|\n",
      "|A1NATT3PN24QWY|  200|\n",
      "|A1K1JW1C5CUSUZ|  179|\n",
      "|A1X8VZWTOG8IS6|  174|\n",
      "|A1S3C5OFU508P3|  158|\n",
      "|A3M174IC0VXOS2|  152|\n",
      "|A2EDZH51XHFA9B|  147|\n",
      "|A2VE83MZF98ITY|  143|\n",
      "|A21NVBFIEQWDSG|  142|\n",
      "|A2NJO6YE954DBH|  141|\n",
      "|A2OJW07GQRNJUT|  129|\n",
      "|A2F6N60Z96CAJI|  118|\n",
      "|A1OX82JPAQLL60|  113|\n",
      "|A281NPSIMI1C2R|  112|\n",
      "+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 121:======>                                                  (1 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|range| count|\n",
      "+-----+------+\n",
      "|    0|303448|\n",
      "|    1|  1213|\n",
      "|    2|   250|\n",
      "|    3|   126|\n",
      "|    4|    47|\n",
      "|    5|    33|\n",
      "|    6|    16|\n",
      "|    7|    10|\n",
      "|    8|     6|\n",
      "|    9|     7|\n",
      "|   10|     8|\n",
      "|   11|     5|\n",
      "|   12|     1|\n",
      "|   14|     4|\n",
      "|   15|     2|\n",
      "|   17|     2|\n",
      "|   20|     2|\n",
      "|   24|     1|\n",
      "|   27|     2|\n",
      "|   30|     1|\n",
      "+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count the number of ratings per user\n",
    "user_id_counts = books_rating.groupBy(\"User_id\").agg(count(\"*\").alias(\"count\"))\n",
    "user_id_counts= user_id_counts.orderBy(user_id_counts[\"count\"].desc())\n",
    "user_id_counts.show(20)\n",
    "result = user_id_counts.groupBy((col('count') / 10).cast('int').alias('range')).count().orderBy('range')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5442765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 129:==========================================>            (17 + 5) / 22]\r"
     ]
    }
   ],
   "source": [
    "# Filter out users that has invalid user_id\n",
    "user_id_counts = user_id_counts.filter(col(\"user_id\").rlike(\"^[a-zA-Z0-9]+$\"))\n",
    "user_id_counts.show(20)\n",
    "\n",
    "# Filter users who has given more than 50 ratings\n",
    "filtered_user_ids = user_id_counts.filter(col(\"count\") >= 1).select(\"User_id\")\n",
    "filtered_user_ids.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2208459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join to filter the ratings\n",
    "filtered_ratings = books_rating.join(filtered_user_ids, \"User_id\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425406be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging both datasets\n",
    "books_merged = filtered_ratings.join(books_data, \"Title\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81106de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the rating column is numeric\n",
    "books_final = books_merged.withColumn(\"rating\", col(\"rating\").cast(\"double\"))\n",
    "books_final_df = books_final.toPandas()\n",
    "print(books_final_df)\n",
    "books_final_df.to_csv('books_final.csv', index=False)\n",
    "books_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bad84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the contents of the DataFrame\n",
    "books_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_final.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989067eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "temp_df = books_final.groupBy(\"rating\").count().toPandas()\n",
    "\n",
    "# Create trace1 for bar chart\n",
    "trace1 = go.Bar(\n",
    "    x=temp_df['rating'],\n",
    "    y=temp_df['count'],\n",
    "    text = temp_df['count'],  # Labels for each bar\n",
    "    marker=dict(color='rgb(255,165,0)', line=dict(color='rgb(0,0,0)', width=1.5))\n",
    ")\n",
    "layout_bar = go.Layout(\n",
    "    template=\"plotly_dark\",\n",
    "    title='RATINGS COUNT',\n",
    "    xaxis=dict(title='Rating'),\n",
    "    yaxis=dict(title='Count')\n",
    ")\n",
    "fig_bar = go.Figure(data=[trace1], layout=layout_bar)\n",
    "fig_bar.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f0e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "\n",
    "# Filter out null values\n",
    "temp_df_filtered = temp_df.dropna(subset=['rating'])\n",
    "\n",
    "def pie_plot(cnt_srs, title):\n",
    "    labels = cnt_srs['rating']\n",
    "    values = cnt_srs['count']\n",
    "    trace = go.Pie(\n",
    "        labels=labels,\n",
    "        values=values,\n",
    "        hoverinfo='percent+value',\n",
    "        textinfo='percent',\n",
    "        textposition='inside',\n",
    "        hole=0.7,\n",
    "        showlegend=True,\n",
    "        marker=dict(\n",
    "            colors=plt.cm.viridis_r(np.linspace(0, 1, len(cnt_srs))),\n",
    "            line=dict(color='#000000', width=2)\n",
    "        )\n",
    "    )\n",
    "    layout = go.Layout(\n",
    "        template=\"plotly_dark\",\n",
    "        title=title\n",
    "    )\n",
    "    fig = go.Figure(data=[trace], layout=layout)\n",
    "    return fig\n",
    "\n",
    "fig_pie = pie_plot(temp_df_filtered, 'Rating Distribution')\n",
    "fig_pie.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfaec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "# Split the categories column into Fiction and Non-Fiction\n",
    "genre_counts = books_final.withColumn(\"Fiction\", when(col(\"categories\") == \"Fiction\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"Non-Fiction\", when(col(\"categories\") != \"Fiction\", 1).otherwise(0)) \\\n",
    "    .groupBy().sum(\"Fiction\", \"Non-Fiction\") \\\n",
    "    .withColumnRenamed(\"sum(Fiction)\", \"Fiction\") \\\n",
    "    .withColumnRenamed(\"sum(Non-Fiction)\", \"Non-Fiction\")\n",
    "genre_counts_pd = genre_counts.toPandas()\n",
    "fiction_label = genre_counts_pd.iloc[0][\"Fiction\"]\n",
    "non_fiction_label = genre_counts_pd.iloc[0][\"Non-Fiction\"]\n",
    "\n",
    "trace1 = go.Bar(\n",
    "    x=[\"Fiction\", \"Non-Fiction\"],\n",
    "    y=[fiction_label, non_fiction_label],\n",
    "    text=[fiction_label, non_fiction_label],  # Labels for each bar\n",
    "    marker=dict(color='rgb(255,165,0)',\n",
    "                line=dict(color='rgb(0,0,0)', width=1.5))\n",
    ")\n",
    "layout = go.Layout(template=\"plotly_dark\", title = 'Count of ratings among Fiction and Non-Fiction Books', xaxis=dict(title='Categories'), yaxis=dict(title='Count'))\n",
    "fig = go.Figure(data=[trace1], layout=layout)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d8990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Split the categories column into Fiction and Non-Fiction\n",
    "genre_counts = books_final.withColumn(\"Category\", when(col(\"categories\") == \"Fiction\", \"Fiction\").otherwise(\"Non-Fiction\"))\n",
    "genre_counts_pie = genre_counts.groupBy('Category').count()\n",
    "genre_counts_pd = genre_counts_pie.toPandas()\n",
    "def pie_plot(cnt_srs, title):\n",
    "    labels = cnt_srs['Category']\n",
    "    values = cnt_srs['count']\n",
    "    trace = go.Pie(\n",
    "        labels=labels,\n",
    "        values=values,\n",
    "        hoverinfo='percent+value',\n",
    "        textinfo='percent',\n",
    "        textposition='inside',\n",
    "        hole=0.7,\n",
    "        showlegend=True,\n",
    "        marker=dict(\n",
    "            colors=plt.cm.viridis_r(np.linspace(0, 1, len(cnt_srs))),\n",
    "            line=dict(color='#000000', width=2)\n",
    "        )\n",
    "    )\n",
    "    layout = go.Layout(\n",
    "        template=\"plotly_dark\",\n",
    "        title=title\n",
    "    )\n",
    "    fig = go.Figure(data=[trace], layout=layout)\n",
    "    return fig\n",
    "fig_pie = pie_plot(genre_counts_pd, 'GENRE Distribution')\n",
    "fig_pie.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fae8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bubbly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3da642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "df1 = books_final.groupBy(['categories', 'publishedYear']).agg(F.mean('rating').alias('User Rating'), F.mean('Price').alias('Price')).toPandas()\n",
    "fig = make_subplots(rows=1, cols=1)\n",
    "for genre in df1['categories'].unique():\n",
    "    df_genre = df1[df1['categories'] == genre]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_genre['User Rating'],\n",
    "        y=df_genre['Price'],\n",
    "        mode='markers',\n",
    "        marker=dict(size=10),\n",
    "        name=genre\n",
    "    ))\n",
    "fig.update_layout(\n",
    "    template=\"plotly_dark\",\n",
    "    title='Bestsellers Amazon',\n",
    "    xaxis_title='User Rating',\n",
    "    yaxis_title='Avg Price',\n",
    "    xaxis=dict(type='log'),  # Log scale for x-axis\n",
    "    showlegend=True\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "df1 = books_final.groupBy(['publishedYear']).agg(F.mean('rating').alias('User Rating'), F.mean('Price').alias('Price')).toPandas()\n",
    "fig = make_subplots(rows=1, cols=1)\n",
    "for year in df1['publishedYear'].unique():\n",
    "    df_genre = df1[df1['publishedYear'] == year]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_genre['User Rating'],\n",
    "        y=df_genre['Price'],\n",
    "        mode='markers',\n",
    "        marker=dict(size=10),\n",
    "        name=str(year)\n",
    "    ))\n",
    "fig.update_layout(\n",
    "    template=\"plotly_dark\",\n",
    "    title='Bestsellers Amazon',\n",
    "    xaxis_title='User Rating',\n",
    "    yaxis_title='Avg Price',\n",
    "    xaxis=dict(type='log'),  # Log scale for x-axis\n",
    "    showlegend=True\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a5b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "genre_counts = books_final.withColumn(\"Fiction\", when(col(\"categories\") == \"Fiction\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"Non-Fiction\", when(col(\"categories\") != \"Fiction\", 1).otherwise(0)) \\\n",
    "    .groupBy().sum(\"Fiction\", \"Non-Fiction\") \\\n",
    "    .withColumnRenamed(\"sum(Fiction)\", \"Fiction\") \\\n",
    "    .withColumnRenamed(\"sum(Non-Fiction)\", \"Non-Fiction\")\n",
    "fiction_data = books_final.filter(col(\"categories\") == \"Fiction\")\n",
    "non_fiction_data = books_final.filter(col(\"categories\") == \"Non Fiction\")\n",
    "temp_df1 = fiction_data.groupBy(\"rating\").count().orderBy(\"rating\").toPandas()\n",
    "temp_df2 = non_fiction_data.groupBy(\"rating\").count().orderBy(\"rating\").toPandas()\n",
    "import plotly.graph_objs as go\n",
    "trace1 = go.Bar(\n",
    "    x=temp_df1['rating'],\n",
    "    y=temp_df1['count'],\n",
    "    name=\"Fiction\",\n",
    "    marker=dict(color='rgb(249, 6, 6)',\n",
    "                line=dict(color='rgb(0,0,0)', width=1.5))\n",
    ")\n",
    "trace2 = go.Bar(\n",
    "    x=temp_df2['rating'],\n",
    "    y=temp_df2['count'],\n",
    "    name=\"Non Fiction\",\n",
    "    marker=dict(color='rgb(26, 118, 255)',\n",
    "                line=dict(color='rgb(0,0,0)', width=1.5))\n",
    ")\n",
    "layout = go.Layout(\n",
    "    template=\"plotly_dark\",\n",
    "    title='RATING BY GENRE',\n",
    "    xaxis=dict(title='Rating'),\n",
    "    yaxis=dict(title='Count')\n",
    ")\n",
    "fig = go.Figure(data=[trace1, trace2], layout=layout)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2afb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Author and count occurrences\n",
    "top_authors = books_final.groupBy('authors').agg(F.avg('rating').alias(\"author_rating\")) \\\n",
    "    .orderBy('author_rating', ascending=False).limit(10)\n",
    "top_authors_pd = top_authors.toPandas()\n",
    "trace1 = go.Bar(\n",
    "    x=top_authors_pd['authors'],\n",
    "    y=top_authors_pd['author_rating'],\n",
    "    marker=dict(color='rgb(255,165,0)',\n",
    "                line=dict(color='rgb(0,0,0)', width=1.5))\n",
    ")\n",
    "layout = go.Layout(\n",
    "    template=\"plotly_dark\",\n",
    "    title='TOP 10 AUTHORS WITH HIGH AVERAGE RATING',\n",
    "    xaxis=dict(title='Author', tickangle=45),\n",
    "    yaxis=dict(title='COUNT')\n",
    ")\n",
    "fig = go.Figure(data=[trace1], layout=layout)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e12b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Author and count occurrences\n",
    "top_publishers = books_final.groupBy('publisher').agg(F.avg('rating').alias(\"publisher_rating\")) \\\n",
    "    .orderBy('publisher_rating', ascending=False).limit(10)\n",
    "top_publishers_pd = top_publishers.toPandas()\n",
    "trace1 = go.Bar(\n",
    "    x=top_publishers_pd['publisher'],\n",
    "    y=top_publishers_pd['publisher_rating'],\n",
    "    marker=dict(color='rgb(255,165,0)',\n",
    "                line=dict(color='rgb(0,0,0)', width=1.5))\n",
    ")\n",
    "layout = go.Layout(\n",
    "    template=\"plotly_dark\",\n",
    "    title='TOP 10 PUBLISHERS WITH HIGH AVERAGE RATING',\n",
    "    xaxis=dict(title='Author', tickangle=45),\n",
    "    yaxis=dict(title='COUNT')\n",
    ")\n",
    "fig = go.Figure(data=[trace1], layout=layout)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6be4f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df1_year = books_final.groupBy('publishedYear').mean()\n",
    "temp_df1_year_pd = temp_df1_year.toPandas()\n",
    "trace1_year = go.Bar(\n",
    "    x=temp_df1_year_pd['publishedYear'],\n",
    "    y=temp_df1_year_pd['avg(rating)'],\n",
    "    marker=dict(color='rgb(255,165,0)',\n",
    "                line=dict(color='rgb(0,0,0)',width=1.5))\n",
    ")\n",
    "layout_year = go.Layout(\n",
    "    template=\"plotly_dark\",\n",
    "    title='AVERAGE REVIEWS OVER THE YEARS',\n",
    "    xaxis=dict(title='Year'),\n",
    "    yaxis=dict(title='Average Reviews')\n",
    ")\n",
    "fig_year = go.Figure(data=[trace1_year], layout=layout_year)\n",
    "fig_year.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70fe0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "\n",
    "# Group by publishedYear and calculate the mean of price\n",
    "temp_df_year = books_final.groupBy('publishedYear').agg(F.mean('Price').alias('avg_price'))\n",
    "temp_df_year_pd = temp_df_year.toPandas()\n",
    "trace1_year = go.Bar(\n",
    "    x=temp_df_year_pd['publishedYear'],\n",
    "    y=temp_df_year_pd['avg_price'],\n",
    "    marker=dict(color='rgb(148, 103, 189)',\n",
    "                line=dict(color='rgb(0,0,0)', width=1.5))\n",
    ")\n",
    "layout_year = go.Layout(\n",
    "    template=\"plotly_dark\",\n",
    "    title='AVERAGE PRICE OVER THE YEARS',\n",
    "    xaxis=dict(title='Year'),\n",
    "    yaxis=dict(title='Average Price')\n",
    ")\n",
    "fig_year = go.Figure(data=[trace1_year], layout=layout_year)\n",
    "fig_year.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2404069",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_final.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b3f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivoting the DataFrame\n",
    "book_pivot = books_final.groupBy(\"Title\").pivot(\"User_id\").avg(\"rating\")\n",
    "\n",
    "# Fill Null values with 0\n",
    "book_pivot = book_pivot.na.fill(0)\n",
    "\n",
    "# Convert pivot table to RDD of (Title, features) tuples\n",
    "sparse_rdd = book_pivot.rdd.map(lambda row: (row[0], Vectors.dense(row[1:])))\n",
    "\n",
    "# Define a schema for the RDD\n",
    "schema = [\"Title\", \"features\"]\n",
    "\n",
    "# Create a DataFrame from the RDD\n",
    "sparse_matrix = spark.createDataFrame(sparse_rdd, schema)\n",
    "sparse_matrix.show()\n",
    "\n",
    "# Normalizing the features\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"norm_features\")\n",
    "norm_features = normalizer.transform(sparse_matrix)\n",
    "\n",
    "# Creating a Bucketed Random Projection LSH model\n",
    "brp = BucketedRandomProjectionLSH(inputCol=\"norm_features\", outputCol=\"hashes\", bucketLength=1.0, numHashTables=10)\n",
    "model = brp.fit(norm_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e62f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provide input book title from the user\n",
    "user_input_title = input(\"Enter the title of the book: \")\n",
    "user_input_title = user_input_title.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88e3b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the input book title is in the DataFrame\n",
    "if book_pivot.filter(col(\"Title\") == user_input_title).count() == 0:\n",
    "    print(f\"Book '{user_input_title}' not found.\")\n",
    "else:\n",
    "    # Extracting the features of the input book\n",
    "    input_book_features = norm_features.filter(col(\"Title\") == user_input_title).select(\"norm_features\").collect()[0][0]\n",
    "\n",
    "# Approximate k nearest neighbors of the input book\n",
    "knn = model.approxNearestNeighbors(norm_features, input_book_features, 6)\n",
    "# Display the recommended books with their IDs\n",
    "recommended_books = knn.filter(col(\"Title\") != user_input_title).limit(5).collect()\n",
    "\n",
    "for book in recommended_books:\n",
    "    print(book[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2362741f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
